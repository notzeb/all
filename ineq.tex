\section{Mechanical procedures}

\subsection{Quadratic inequalities: Keep completing the square!}

Suppose someone hands you a quadratic polynomial in several variables, such as
\[
x^2 + 2xy - 2xz + 2y^2 + 2yz + 6z^2 - z + 1,
\]
and asks you to check whether it is always $\ge 0$. How do you do it?

The trick to this is a slight generalization of the high school procedure known as ``completing the square'', which I like to call ``keep completing the square'' (I stumbled on this method after meditating on what the Cholesky decomposition really \emph{meant} in terms of quadratic polynomials). We start by trying to write down a square that agrees with our polynomial at least as far as $x$ is concerned, that is, we try to solve the equation
\[
(x + Ay + Bz + C)^2 = x^2 + 2xy - 2xz + ...,
\]
for $A,B,C$ (and ignoring the $...$, since it doesn't involve $x$). In this case, we can take $A = 1, B = -1, C = 0$, and we get
\[
(x + y - z)^2 = x + 2xy - 2xz + y^2 - 2yz + z^2.
\]
Since that doesn't completely match our polynomial, we look at the difference:
\[
(x^2 + 2xy - 2xz + 2y^2 + 2yz + 6z^2 - z + 1) - (x + y - z)^2 = y^2 + 4yz + 5z^2 - z + 1.
\]
Now we complete the square again, this time with $y$, and so on. Writing the whole process in one string of equalities, we get
\begin{align*}
x^2 + 2xy - 2xz + 2y^2 + 2yz + 6z^2 - 2z + 1 &= (x + y - z)^2 + y^2 + 4yz + 5z^2 - z + 1\\
&= (x + y - z)^2 + (y + 2z)^2 + z^2 - z + 1\\
&= (x + y - z)^2 + (y + 2z)^2 + (z - \tfrac{1}{2})^2 + \tfrac{3}{4},
\end{align*}
and this is clearly positive, since it is a sum of squares.

Let's do a more complicated example (the previous example was clearly chosen to let you avoid taking any square roots). What if we are faced with something like
\[
6x^2 - 4xy + 2xz + 3y^2 - 4yz + 2z^2?
\]
At the very first step, it seems like we'll have to take the square root of $6$. What a mess! Here's how to avoid the mess: instead of starting with a square like
\[
(\sqrt{6}x + Ay + Bz)^2,
\]
instead we start by looking for something like
\[
6(x + Ay + Bz)^2.
\]
Now we can find $A, B$ by simple division, and we get $A = -\frac{1}{3}, B = \frac{1}{6}$. Continuing, we get
\begin{align*}
6x^2 - 4xy + 2xz + 3y^2 - 4yz + 2z^2 &= 6(x - \tfrac{1}{3}y + \tfrac{1}{6}z)^2 + \tfrac{7}{3}y^2 - \tfrac{10}{3}yz + \tfrac{11}{6}z^2\\
&= 6(x - \tfrac{1}{3}y + \tfrac{1}{6}z)^2 + \tfrac{7}{3}(y - \tfrac{5}{7}z)^2 + \tfrac{9}{14}z^2,
\end{align*}
which is again obviously positive since it has been written as a sum of squares with positive coefficients. (By the way, I came up this polynomial by expanding out $(x-y)^2 + (x+y-z)^2 + (2x-y+z)^2$ - so we see that there can be multiple ways to write the same polynomial as a sum of squares. If we had processed the variables in a different order, we could come up with yet another way to write it as a sum of squares!)

What happens if we try to do this to a quadratic polynomial which \emph{isn't} always $\ge 0$? Obviously, something has to go wrong. Let's try the polynomial
\[
x^2 - 4xy + 2xz + y^2 - 2yz + 2z^2.
\]
The first step goes just fine: we get
\[
x^2 - 4xy + 2xz + y^2 - 2yz + 2z^2 = (x - 2y + z)^2 - 3y^2 + 2yz + z^2.
\]
But now we have a problem: the coefficient of $y^2$ is negative. Could our polynomial still be $\ge 0$? Maybe the $z^2$ and the $(x - 2y + z)^2$ somehow always conspire to be larger than $3y^2$? Nope! To see why, just set $z$ to $0$, and choose $x$ to make $x - 2y + z$ equal to $0$, for instance, take $z = 0, y = 1, x = 2$.

In the previous example, we had a problem because the coefficient of $y^2$ was negative. What if the coefficient of $y^2$ comes out to exactly $0$? For an example, let's consider the polynomial
\[
x^2 - 2xy - 2xz + y^2 - 2yz + 10z^2.
\]
After the first step, we get
\[
x^2 - 2xy - 2xz + y^2 - 2yz + 2z^2 = (x - y - z)^2 - 4yz + 9z^2.
\]
To show that this sometimes goes negative, we will take $z$ to be whatever nonzero value we like - say, take $z = 1$ - and then pick $y$ to make $-4yz + 9z^2$ come out negative (we can do this since, for any fixed nonzero $z$, $-4yz + 9z^2$ is a linear function of $y$ with a nonzero $y$-coefficient), and finally pick $x$ to make $x-y-z$ equal to $0$. For instance, we can take $z = 1, y = 3, x = 4$.

At the end of the day, we have a procedure that starts with a quadratic polynomial in any number of variables, and either writes it as a sum of squares with positive coefficients, or spits out a point where it is negative! We summarize in the following theorem.

\begin{thm} Suppose that $Q(x_1, ..., x_n) = \sum_{i,j} a_{ij} x_ix_j + \sum_i a_ix_i + a$, where $a_{ij}, a_i, a$ are some coefficients. Then either we can write $Q$ in the form
\[
Q(x_1, ..., x_n) = \sum_{i=1}^n c_i(x_i + b_{i(i+1)}x_{i+1} + \cdots + b_{in}x_n + b_i)^2 + c
\]
with $c_i \ge 0$ for all $i$ and $c \ge 0$, or else we can find a point $(x_1, ..., x_n)$ such that $Q(x_1, ..., x_n) < 0$.
\end{thm}

In the case of homogeneous quadratic polynomials, people often like to represent their coefficients in a symmetric matrix. In the three variable case, the matrix
\[
\begin{bmatrix} a & b & d\\
b & c & e\\
d & e & f
\end{bmatrix}
\]
corresponds to the polynomial
\[
ax^2 + 2bxy + cy^2 + 2dxz + 2eyz + fz^2.
\]
Why the random factors of $2$? This is because we have the nice formula
\[
\begin{bmatrix} x & y & z \end{bmatrix} \begin{bmatrix} a & b & d\\ b & c & e\\ d & e & f \end{bmatrix} \begin{bmatrix} x\\ y\\ z\end{bmatrix} = ax^2 + 2bxy + cy^2 + 2dxz + 2eyz + fz^2.
\]

When we follow the ``keep completing the square'' procedure for this general three variable homogeneous quadratic, we get
\begin{align*}
ax^2 + 2bxy + cy^2 + 2dxz + 2eyz + fz^2 &= a(x + \tfrac{b}{a}y + \tfrac{d}{a}z)^2 + \tfrac{ac-b^2}{a}y^2 + 2\tfrac{ae-bd}{a}yz + \tfrac{af-d^2}{a}z^2\\
&= a(x + \tfrac{b}{a}y + \tfrac{d}{a}z)^2 + \tfrac{ac-b^2}{a}(y + \tfrac{ae-bd}{ac-b^2}z)^2 + \tfrac{(af-d^2)(ac-b^2)-(ae-bd)^2}{a(ac-b^2)}z^2\\
&= a(x + \tfrac{b}{a}y + \tfrac{d}{a}z)^2 + \tfrac{ac-b^2}{a}(y + \tfrac{ae-bd}{ac-b^2}z)^2 + \tfrac{acf + 2bde - ae^2 - b^2f - cd^2}{ac-b^2}z^2.
\end{align*}
Curiously, the coefficients in that last formula happen to be ratios of determinants:
\begin{align*}
\det \begin{bmatrix} a\end{bmatrix} &= a,\\
\det \begin{bmatrix} a & b\\ b & c\end{bmatrix} &= ac - b^2,\\
\det \begin{bmatrix} a & b & d\\ b & c & e\\ d & e & f \end{bmatrix} &= acf + 2bde - ae^2 - b^2f - cd^2.
\end{align*}
So we've proved that a three variable homogeneous quadratic is $\ge 0$ if those three determinants are all positive!

\bigskip

\begin{exer} Generalize this determinant formula to any number of variables.
\end{exer}


\subsection{Systems of linear inequalities: Fourier-Motzkin Elimination}

Suppose that someone hands you a system of linear inequalities in several variables, such as
\begin{align*}
2x + y &\le 2z,\\
x + z &\le 2y + 1,\\
x + 3 &\le 2y,\\
3x &\le y + z,\\
y + z &\le 2x + 3,
\end{align*}
and asks you whether or not this system of inequalities has a solution. In fact, to make it more interesting, suppose they ask you to find all possible values of $x$ which can occur in a solution $(x,y,z)$ to this system of inequalities. How do you do it?

There are several different ways to solve this sort of problem, but the simplest and most direct method is known as \emph{Fourier-Motzkin elimination}. The idea is to pick one of the variables and to \emph{eliminate} it, getting a system of linear inequalities in the remaining variables which captures every last bit of information that doesn't involve the eliminated variable.

Let's start by trying to eliminate the variable $z$ from our system of linear inequalities. To start, we rearrange all of our inequalities into three different categories based on how they involve $z$.
\begin{itemize}
\item Some of the inequalities give us \emph{lower bounds} on $z$. In our example, the first and fourth inequalities give us lower bounds on $z$, and we rewrite them to make this more obvious:
\begin{align*}
x + y/2 &\le z,\\
3x - y &\le z.
\end{align*}

\item Some of the inequalities give us \emph{upper bounds} on $z$. In our example, the second and fifth inequalities give us lower bounds on $z$, and we rewrite them to make this more obvious:
\begin{align*}
z &\le -x + 2y + 1,\\
z &\le 2x - y + 3.
\end{align*}

\item Some of the inequalities don't involve $z$ at all. In our example, the third inequality didn't involve $z$ at all:
\[
x + 3 \le 2y.
\]
\end{itemize}
To eliminate $z$, we need to figure out which pairs of values $(x,y)$ allow us to pick a $z$ which satisfies all three types of inequalities above. If $(x,y)$ satisfy all of the inequalities that don't involve $z$, then obviously the only possible thing that can go wrong when we try to find a value for $z$ is that one of the lower bounds for $z$ might be bigger than one of the upper bounds for $z$. As long as each lower bound for $z$ is at most as large as each upper bound for $z$, we will be fine. So the new system of inequalities, after we eliminate $z$, is just
\begin{align*}
x + 3 &\le 2y,\\
x + y/2 &\le -x + 2y + 1,\\
x + y/2 &\le 2x - y + 3,\\
3x - y &\le -x + 2y + 1,\\
3x - y &\le 2x - y + 3.
\end{align*}
As long as $x$ and $y$ satisfy this new system of inequalities, we can pick any $z$ which satisfies
\[
\max(x + y/2, 3x-y) \le z \le \min(-x + 2y + 1, 2x - y + 3)
\]
to solve the original system of linear inequalities. We have successfully eliminated the variable $z$!

Now we slightly rearrange each one of our new linear inequalities to clear denominators and so on:
\begin{align*}
x + 3 &\le 2y,\\
4x &\le 3y + 2,\\
3y &\le 2x + 6,\\
4x &\le 3y + 1,\\
x &\le 3.
\end{align*}
The second of these is clearly redundant, so we can forget about it. Now we want to eliminate $y$ from our new system of linear inequalities. Once again, we divide our inequalities into three different categories based on how they involve $y$.
\begin{itemize}
\item Some of the inequalities give us \emph{lower bounds} on $y$. In our new system of linear inequalities, the first and fourth inequalities give us lower bounds on $y$, and we rewrite them to make this more obvious:
\begin{align*}
x/2 + 3/2 &\le y,\\
4x/3 - 1/3 &\le y.
\end{align*}

\item Some of the inequalities give us \emph{upper bounds} on $y$. In our new system of linear inequalities, the third inequality gives us a lower bound on $y$, and we rewrite it to make this more obvious:
\[
y \le 2x/3 + 2.
\]

\item Some of the inequalities don't involve $y$ at all. In our new system of linear inequalities, the fifth inequality didn't involve $y$ at all:
\[
x \le 3.
\]
\end{itemize}
Once again, we keep every inequality that doesn't involve $y$ at all, and we compare every lower bound on $y$ to every upper bound on $y$. This gives us the following system of linear inequalities involving only $x$:
\begin{align*}
x &\le 3,\\
x/2 + 3/2 &\le 2x/3 + 2,\\
4x/3 - 1/3 &\le 2x/3 + 2.
\end{align*}
As long as $x$ satisfies this system of linear inequalities, we can pick any $y$ which satisfies
\[
\max(x/2 + 3/2, 4x/3 - 1/3) \le y \le 2x/3 + 2.
\]
We have now successfully eliminated both $y$ and $z$!

Now we slightly rearrange our inequalities on $x$:
\begin{align*}
x &\le 3,\\
0 &\le x + 3,\\
2x &\le 7.
\end{align*}
The third inequality on $x$ is clearly redundant, and we see that the possible values for $x$ are exactly the values of $x$ between $-3$ and $3$!

Now suppose that we want to convince a skeptical friend that the possible values for $x$ are exactly the values between $-3$ and $3$. First, we verify that we really can fill in values for $y$ and $z$ when $x$ is $-3$ or $3$. If $x$ is $-3$, then $y$ can be any value which satisfies
\[
0 = \max(x/2 + 3/2, 4x/3 - 1/3) \le y \le 2x/3 + 2 = 0,
\]
so we have to choose $y = 0$. Then $z$ can be any value which satisfies
\[
-3 = \max(x + y/2, 3x-y) \le z \le \min(-x + 2y + 1, 2x - y + 3) = -3,
\]
so we have to choose $z = -3$. So we show our friend that $(x,y,z) = (-3, 0, -3)$ solves our system of linear inequalities. A similar calculation leads us to the fact that $(x,y,z) = (3, 4, 5)$ also solves our system of linear inequalities. By taking convex combinations of these two solutions, we see that for any $x \in [-3, 3]$, the point
\[
(x,y,z) = (x, 2x/3 + 2, 4x/3 + 1)
\]
will solve our system of linear inequalities.

How do we convince our skeptical friend that $x$ can't be bigger than $3$ or smaller than $-3$? We just work backwards through our elimination procedure to see how we derived the inequalities $x \le 3$ and $0 \le x + 3$. For the upper bound $x \le 3$, we see that this was one of the bounds which didn't involve $y$ at all, after we had eliminated $z$, and that we had obtained it by simplifying the inequality
\[
3x - y \le 2x - y + 3.
\]
This inequality was obtained by comparing the lower bound $3x-y \le z$, which corresponded to the fourth inequality in our original system of inequalities, to the upper bound $z \le 2x - y + 3$, which corresponded to the fifth inequality of our original system of inequalities. So we can simply tell our friend that we added together the inequalities
\begin{align*}
3x &\le y + z,\\
y + z &\le 2x + 3
\end{align*}
and simplified, to deduce the upper bound $x \le 3$. Note that each of these two inequalities has equality when $(x,y,z) = (3,4,5)$.

As for the lower bound $0 \le x + 3$, this was a simplification of the inequality
\[
x/2 + 3/2 \le 2x/3 + 2
\]
which we found by rearranging and multiplying both sides by $6$. That inequality, in turn, was obtained by comparing the lower bound $x/2 + 3/2 \le y$ to the upper bound $y \le 2x/3 + 2$, and these two inequalities were rescaled versions of the inequalities $x + 3 \le 2y$ and $3y \le 2x + 6$. So the inequality $0 \le x + 3$ follows from adding together the two inequalities
\begin{align*}
3 \times \Big(x + 3 &\le 2y\;\;\;\;\;\;\Big),\\
2 \times \Big(\;\;\;\;3y &\le 2x + 6\Big).
\end{align*}
The inequality $x+3 \le 2y$ was one of the original inequalities which didn't involve $z$, while the inequality $3y \le 2x + 6$ was a rescaled version of the inequality $x + y/2 \le 2x - y + 3$. The inequality $x + y/2 \le 2x - y + 3$ was built out of the inequalities $2x + y \le 2z$ and $y + z \le 2x + 3$ by eliminating $z$ - that is, we derived it by adding the inequalities
\begin{align*}
2x + y &\le 2z,\\
2\times\Big(\;\;y + z &\le 2x + 3\Big).
\end{align*}
All together, we see that the inequality $0 \le x + 3$ was derived from the original system of linear inequalities by adding together the following three multiples of the first, third, and fifth inequalities from our original system of linear inequalities:
\begin{align*}
2 \times \Big(2x + y &\le 2z\;\;\;\;\;\Big),\\
3 \times \Big(\;x + 3 &\le 2y\;\;\;\;\;\Big),\\
4\times\Big(\;y + z &\le 2x + 3\Big).
\end{align*}
Note that each of these three inequalities has equality when $(x,y,z) = (-3,0,-3)$.

It's also possible to deal with mixtures of linear inequalities and linear equations - in fact, as long as there is at least one nontrivial linear equation around, we can use it to eliminate a variable directly, as in Gaussian elimination. We can summarize the main idea behind this procedure in the following result.

\begin{thm}[Fourier-Motzkin Elimination] Suppose that we have a system $S_n$ of $m$ linear inequalities and linear equations in $n$ unknowns $x_1, ..., x_n$. Then we can find a new system $S_{n-1}$ of at most $\max(m, m^2/4)$ linear inequalities and linear equations in the $n-1$ unknowns $x_1, ..., x_{n-1}$, with the following properties:
\begin{itemize}
\item the values $(x_1, ..., x_{n-1})$ satisfy the new system $S_{n-1}$ if and only if there is some $x_n$ such that the values $(x_1, ..., x_{n-1}, x_n)$ satisfy the original system $S_n$, and

\item every linear inequality or linear equation in the new system $S_{n-1}$ is either one of the inequalities/equations from $S_n$ which did not involve the variable $x_n$, or can be written as a weighted combination of two inequalities/equations from $S_n$ with weights chosen to cancel out the coefficient of the variable $x_n$.
\end{itemize}
\end{thm}

\begin{cor} If a system of linear inequalities and linear equations has no solutions, then by summing multiples of these inequalities and equations, we can derive a false inequality
\[
a \le b,
\]
where $a,b$ are constants with $a > b$. Equivalently, in this case we can derive the false inequality
\[
1 \le 0
\]
by summing multiples of our inequalities and equations.
\end{cor}

\begin{cor} If a system $S$ of linear inequalities and linear equations in the variables $x_1, ..., x_n$ implies the inequality $x_1 \le c$ for some constant $c$, then this inequality can be derived by summing multiples of the inequalities and equations from $S$.

If there is also a solution $x^* = (x_1^*, ..., x_n^*)$ to the system $S$ which has $x_1^* = c$, then every single inequality which occurs in the sum which we used to derive the inequality $x_1 \le c$ must have equality at the point $x^*$.
\end{cor}

By changing variables, we can prove the following stronger-looking result.

\begin{cor} If a system $S$ of linear inequalities and linear equations in the variables $x_1, ..., x_n$ implies a linear inequality $\sum_i a_i x_i \le c$, then this inequality can be derived by summing multiples of the inequalities and equations from $S$.
\end{cor}

If the number $m$ of inequalities and the number $n$ of variables in our original system of linear inequalities are both very large, then the systems of inequalities produced by Fourier-Motzkin elimination can grow out of control. The \emph{simplex method} is a better method for solving larger systems - the idea is to examine points where $n$ of the inequalities have equality, and to compare them to ``neighboring'' points which have equality at a slightly different collection of $n$ of the inequalities, where $n-1$ of the inequalities are the same as before and one is new. More advanced procedures, such as Khachiyan's \emph{ellipsoid method} and Karmarkar's \emph{interior point algorithm} are based on finding approximate solutions to high accuracy and then rounding them - the ellipsoid method is the basis of an important theoretical result about the computational complexity of convex optimization, and interior point algorithms are fast even for enormous problems.

When discussing large systems of linear inequalities, it's convenient to use the notation of linear algebra. We package our collection of variables $x_1, ..., x_n$ into a column vector $x \in \RR^n$, and we package the system of $m$ linear inequalities into the inequality
\[
Ax \le b,
\]
where $A \in \RR^{m\times n}$ is an $m\times n$ matrix whose $m$ rows correspond to the individual inequalities, and $b \in \RR^m$ is a column vector whose entries correspond to the constants which show up in the linear inequalities. If $y \in \RR^m$ is a column vector of \emph{weights} which satisfies $y \ge 0$, then the weighted combination of the inequalities which corresponds to $y$ is given by
\[
y^TAx \le y^Tb,
\]
where $y^T$ is the \emph{transpose} of $y$ (which is a row vector - note that $y^Tb$ is just another way of writing the dot product $y \cdot b$). We can rephrase what we have proved so far in this language, as follows.

\begin{thm}[Theorem of the Alternatives] The system of inequalities $Ax \le b$ has no solution $x \in \RR^n$ if and only if there is some vector $y \in \RR^m$ such that
\begin{itemize}
\item $y \ge 0$,
\item $A^Ty = 0$ (or equivalently $y^TA = 0^T$), and
\item $b^Ty < 0$ (or equivalently $y^Tb < 0$).
\end{itemize}
\end{thm}

An alternative standard way of expressing systems of linear equations and inequalities, which is more useful in some applications, is to introduce new variables which correspond to the amount of ``slack'' that we have in each inequality - so each new variable corresponds to the difference between the right hand side and the left hand side of one of our original inequalities - and to work out a system of equations that has to be satisfied by these ``slack'' variables. This gives us a possibly complicated system of equations, together with a very simple system of inequalities:
\begin{align*}
Ax &= b,\\
x &\ge 0.
\end{align*}
The same argument that gave us the Theorem of the Alternatives gives us an analogous result for systems of this form, known as Farkas' Lemma.

\begin{lem}[Farkas Lemma] The system $Ax = b, x \ge 0$ has no solution if and only if there is some vector $y$ such that $A^Ty \ge 0$ and $b^Ty < 0$.
\end{lem}

\begin{comment}
% From Michel Goemans notes with minor edits from me, doesn't really fit here
\subsubsection{Linear Programming Basics}

A linear program (LP) is the problem of minimizing or maximizing a linear
function over a polyhedron:
\begin{align*}
\mbox{maximize } &  c^Tx\\
\mbox{subject to: } & Ax \leq b,\tag{P}
\end{align*}
where $A\in \RR^{m\times n}$, $b\in \RR^m$, $c\in \RR^n$ and the
variables $x$ are in $\RR^n$. 
Any $x$ satisfying $Ax\leq b$ is said to be {\it feasible}. If no $x$
satisfies $Ax\leq b$, we say that the linear program is {\it
infeasible}, and its optimum value is $-\infty$ (as we are maximizing
over an empty set). If the objective function value of the linear
program can be made arbitrarily large, we say that the linear program
is {\it unbounded} and its optimum value is $+\infty$; otherwise it is
{\it bounded}. If it is neither infeasible, nor unbounded, then its
optimum value is finite.

Other equivalent forms involve equalities as well, or nonnegative
constraints $x\geq 0$. One version that is often considered when
discussing algorithms for linear programming (especially the simplex
algorithm) is $\min\{c^Tx: Ax=b, x\geq 0\}$. 

Another linear program, {\it dual} to $(P)$, plays a crucial role:
\begin{align*}
\mbox{minimize } &  b^Ty\\
\mbox{subject to: } & A^Ty=c,\tag{D}\\
& y\geq 0. 
\end{align*}
$(D)$ is the dual and $(P)$ is the {\it primal}. The terminology for
the dual is similar. If $(D)$ has no feasible solution, it is said to
be {\it infeasible} and its optimum value is $+\infty$ (as we are
minimizing over an empty set). If $(D)$ is unbounded (i.e. its value
can be made arbitrarily negative) then its optimum value is
$-\infty$. 

The primal and dual spaces should not be confused. If $A$ is $m \times
n$ then we have $n$ primal variables and $m$ dual variables.

{\bf Weak duality} is clear: For any feasible solutions $x$ and $y$ to
$(P)$ and $(D)$, we have $c^Tx\leq b^Ty$. Indeed,
\[
c^Tx=y^TAx\leq b^Ty.
\]
The dual was precisely built to get an upper
bound on the value of any primal solution. For example, to get the inequality
$y^TAx \leq b^Ty$, we need $y\geq 0$ since we know that $Ax\leq
b$. In particular, weak duality implies that if the primal is
unbounded then the dual must be infeasible. 

{\bf Strong duality} is the most important result in linear
programming; it says that we can prove the optimality of a primal
solution $x$ by exhibiting an optimum dual solution $y$. 
\begin{thm}[Strong Duality]
Assume that $(P)$ and $(D)$ are feasible, and let $z^*$ be the optimum
value of the primal and $w^*$ the optimum value of the dual. Then
$z^*=w^*$. 
\end{thm}

One proof of strong duality is obtained by writing a big system of
inequalities in $x$ and $y$ which says that (i) $x$ is primal
feasible, (ii) $y$ is dual feasible and (iii) $c^Tx\geq b^Ty$. Then use
the Theorem of the Alternatives to show that the infeasibility of this
system of inequalities would contradict the feasibility of either
$(P)$ or $(D)$. 

\begin{proof}
  Let $x^*$ be a feasible solution to the primal, and $y^*$ be a
  feasible solution to the dual. The proof is by contradiction.
  Because of weak duality, this means that there are no solution $x\in
  \RR^n$ and $y\in \RR^m$ such that
$$\left\{ \begin{array}{lll} Ax & & \leq b \\ & A^T y & =c \\ & -Iy &
    \leq 0 \\ -c^Tx & + b^Ty & \leq 0 \end{array} \right.$$
By a variant of the Theorem of the Alternatives or Farkas' lemma (for
the case when we have a combination of inequalities and equalities),
we derive that there must exist $s\in \RR^m$, $t\in \RR^n$,
$u\in \RR^m$, $v\in \RR$ such that:
\begin{align*}
s &\geq 0 \\
u &\geq 0 \\
v &\geq 0 \\
A^Ts-vc &= 0 \\
At -u + vb &= 0 \\
b^Ts + c^Tt &< 0.
\end{align*}
We distinguish two cases. 

\paragraph{Case 1: $v=0$.} Then $s$ satisfies $s\geq 0$ and
$A^Ts=0$. This means that, for any $\alpha\geq 0$, $y^*+\alpha s$ is
feasible for the dual. Similarly, $At=u \geq 0$ and therefore, for any
$\alpha\geq 0$, we have that $x^*-\alpha t$ is primal feasible. By
weak duality, this
means that, for any $\alpha\geq 0$, we have $$c^T(x^*-\alpha t) \leq
b^T(y^*+\alpha s)$$ or $$c^Tx^*-b^Ty^* \leq \alpha (b^Ts+c^Tt).$$
The right-hand-side tend to $-\infty$ as $\alpha$ tends to $\infty$,
and this is a contradiction as the left-hand-side is fixed. 

\paragraph{Case 2: $v>0$.} By dividing throughout by $v$ (and renaming
all the variables), we get that
there exists $s\geq 0$, $u\geq 0$ with 
\begin{eqnarray*}
A^Ts & = & c \\
At-u & = & -b \\
b^Ts + c^T t & < & 0.
\end{eqnarray*}
This means that $s$ is dual feasible and $-t$ is primal feasible, and
therefore by weak duality $c^T(-t) \leq b^Ts$ contradicting $b^Ts+c^Tt<0$. 
\end{proof}

We can also prove strong duality directly using Fourier-Motzkin elimination.

\begin{proof} Suppose that $c \ne \vec{0}$ (if $c = \vec{0}$, then the theorem follows from the Theorem of Alternatives). Then there is an invertible $n\times n$ matrix $L$ whose first row is $c^T$. Letting $z = Lx$, we see that we can rewrite our linear program in terms of $z$ as
\begin{align*}
\mbox{maximize } &  z_1\\
\mbox{subject to: } & AL^{-1}z \leq b.\tag{P}
\end{align*}
The dual linear program is given by
\begin{align*}
\mbox{minimize } &  b^Ty\\
\mbox{subject to: } & (AL^{-1})^Ty=e_1\tag{D}\\
& y\geq 0,
\end{align*}
where $e_1$ is the first basis vector. Multiplying both sides of $(AL^{-1})^Ty=e_1$ with $L^T$ on the left, we see that
\[
(AL^{-1})^Ty=e_1 \;\;\; \iff \;\;\; A^Ty = c,
\]
so the dual linear program is unchanged.

Now we apply Fourier-Motzkin elimination to the system $AL^{-1}z \le b$ to eliminate the variables $z_2, ..., z_n$, getting a system of linear inequalities in $z_1$, each of which is formed by some nonnegative linear combination of the rows of the system $(AL^{-1})z \le b$. We can divide these inequalities on $z_1$ into three types: upper bounds on $z_1$, lower bounds on $z_1$, and inequalities not involving $z_1$ at all. The inequalities not involving $z_1$ at all must be \emph{true} inequalities between constants by the assumption that the primal problem was feasible, and for the same reason every upper bound on $z_1$ must be at least as large as every lower bound on $z_1$. By the assumption that the dual problem was feasible, we see that there is at least one upper bound on $z_1$. So the maximum value $z_1 = c^Tx$ can take on our polytope is equal to the minimum upper bound which can be proved on $z_1$ by adding together some nonnegative linear combination of the rows of the system $Ax = AL^{-1}z \le b$.
\end{proof}

\begin{exer} Show that the dual of the dual is the primal.
\end{exer}

\begin{exer} Show that we only need either the primal or the dual to be
  feasible for strong duality to hold. More precisely, if the primal
  is feasible but the dual is infeasible, prove that the primal will
  be unbounded, implying that $z^*=w^*=+\infty$.
\end{exer}

Looking at $c^Tx=y^TAx\leq b^Ty$, we observe that to get equality
between $c^Tx$ and $b^Ty$, we need {\it complementary slackness}:

\begin{thm}[Complementary Slackness]
If $x$ is feasible in $(P)$ and $y$ is feasible in $(D)$ then $x$ is
optimum in $(P)$ and $y$ is optimum in $(D)$ if and only if for all
$i$ either $y_i=0$ or $\sum_j a_{ij} x_j =b_i$ (or both). 
\end{thm}
\end{comment}


\subsection{Single-variable polynomials: Sturm chains}

Suppose you are handed a single-variable polynomial, such as
\[
p(x) = x^3 - 6x^2 + 4x + 12
\]
and you are asked to determine whether this polynomial is positive for all $x$ between $-1$ and $2$. How can you accomplish this?

As it turns out, there is a general procedure which allows you to work out the exact number of real roots of any real polynomial in any half-open interval $(a,b]$, using only pencil and paper. The only special fact we will need about the real numbers (as opposed to, say, the rational numbers) is the following weak version of the intermediate value theorem.

\begin{thm}[Intermediate Value Theorem for Polynomials]\label{real-ivt-poly} If $p(x)$ is a real polynomial, and if $p(a)$ and $p(b)$ have opposite signs for some real values $a < b$, then there is some real number $c \in (a,b)$ such that $p(c) = 0$.
\end{thm}
\begin{proof} This is a special case of the intermediate value theorem, which applies to all continuous functions (the fact that polynomials are continuous follows from the binomial theorem and the triangle inequality). The standard proof is as follows: supposing without loss of generality that $p(a) < 0$ and $p(b) > 0$, we define the set $S \subseteq [a,b]$ by
\[
S = \{x \in [a,b] \mid p(x) \le 0\}.
\]
Since $S$ is a nonempty, bounded subset of $\RR$, it has a supremum (by the defining property of the real numbers - in some developments this is an axiom, in others it is proved in terms of a particular construction of the real number system), so we take $c = \sup S$. Since $p$ is continuous, we must have $p(c) \le 0$, since there are values $x$ which are arbitrarily close to $c$ satisfying $p(x) \le 0$. From $p(c) \le 0$ we conclude $c \ne b$, so for every positive $\epsilon < b-c$ we have $p(c+\epsilon) > 0$ (otherwise $c$ would be less than $\sup S$), and applying the continuity of $p$ once more we see that $p(c) \ge 0$ as well, so we must have $p(c) = 0$.
\end{proof}

Versions of Theorem \ref{real-ivt-poly} are true for other number systems as well, such as the collection of \emph{algebraic} real numbers (recall that a number is called algebraic if it is a root of a polynomial which has integer coefficients), or certain number systems involving infinitesimals or power series. An ordered number system which satisfies a version of Theorem \ref{real-ivt-poly} is called a \emph{real closed field}, and for the purpose of studying algebraic inequalities, all real closed fields are essentially indistinguishable from one another.

We will also need a basic algebraic fact about derivatives, which follows from the binomial theorem and is true in any ordered number system.

\begin{prop} If the polynomial $p(x)$ is given by
\[
p(x) = \sum_{i=0}^d a_i x^i
\]
and its \emph{derivative} $p'(x)$ is defined by
\[
p'(x) = \sum_{i=1}^{d} ia_i x^{i-1},
\]
then there is a two-variable polynomial $r(x,y)$ such that
\[
p(x + y) = p(x) + y\cdot p'(x) + y^2\cdot r(x,y).
\]

In particular, if $p'(x) > 0$, then there is some $\epsilon > 0$ such that $p$ is strictly increasing in the interval $(x-\epsilon, x+\epsilon)$.
\end{prop}

Ok, back to the original problem: we want to count the number of roots of $p(x)$ between $-1$ and $2$. A good starting point is to compute $p(-1)$ and $p(2)$ (which come out to $p(-1) = 1$ and $p(2) = 4$, in this example problem), and to check whether they have the same sign or not. Since $p(-1)$ and $p(2)$ are both positive, it seems like we can conclude that $p(x)$ has an even number of roots between $-1$ and $2$. Or can we?

In order to definitively conclude that $p$ has an even number of roots between $-1$ and $2$ from the fact that $p(-1)$ and $p(2)$ are both positive, we need to either be certain that $p$ doesn't have any double (or triple, etc.) roots between $-1$ and $2$, or to be careful to count any multiple roots ``with multiplicity''. For instance, the polynomial
\[
q(x) = (x - 1)^2(x+2) = x^3 - 3x + 2
\]
has $q(-1) = 4$ and $q(2) = 4$, and it has a double root at $x = 1$. In order the detect this multiple root, we use the fact that any multiple root of a polynomial $q(x)$ will also be a root of the derivative $q'(x)$, e.g.:
\begin{align*}
q'(x) &= 2(x-1)(x+2) + (x-1)^2\\
&= (x-1)(2(x+2) + (x-1))\\
&= 3(x^2 - 1).
\end{align*}
If we didn't already know the factorization of $q(x)$, then we could use the Euclidean algorithm to compute the gcd of $q(x)$ and $q'(x)$:
\begin{align*}
\gcd(q(x), q'(x)) &= \gcd(x^3 - 3x + 2,\ 3(x^2 - 1))\\
&= \gcd(-2(x-1),\ 3(x^2 - 1))\\
&= \gcd(-2(x-1),\ 0)\\
&= x-1.
\end{align*}
Using the Euclidean algorithm several times, it is always possible to split any polynomial into pieces which correspond to the roots of various multiplicities. We state this result semi-formally below.

\begin{prop}\label{prop-poly-multiplicity} If $p(x)$ is a polynomial with coefficients from a real closed field, then we can use the Euclidean algorithm several times to write $p$ in the form
\[
p(x) = c\cdot p_1(x)^{m_1}p_2(x)^{m_2}\cdots p_k(x)^{m_k},
\]
where $c$ is a constant, each $p_i(x)$ has no common factor with its derivative $p_i'(x)$, and no $p_i(x)$ has a common factor with $p_j(x)$ for any $i \ne j$.
\end{prop}

So we only need to worry about the case where $p(x)$ has no common factor with its derivative $p'(x)$ (such a polynomial is called \emph{squarefree}). Let's check that our running example $p(x) = x^3 - 6x^2 + 4x + 12$ has this property:
\begin{align*}
\gcd(p(x), p'(x)) &= \gcd(x^3 - 6x^2 + 4x + 12,\ 3x^2 - 12x + 4)\\
&= \gcd(-(4/3)(4x - 11),\ 3x^2 - 12x + 4)\\
&= \gcd(-(4/3)(4x - 11),\ -101/16)\\
&= 1.
\end{align*}
So now we can conclude that $p(x)$ has an even number of roots between $-1$ and $2$, and that the issue of multiplicity is not relevant to in this case. It seems that looking at the derivative $p'(x)$ is generally helpful for thinking about the roots, so we take a peek at the values of $p'(-1)$ and $p'(2)$: in this example, we have $p'(-1) = 19$ and $p'(2) = -8$, so $p(x)$ is increasing around $x = -1$ and $p(x)$ is decreasing around $x = 2$. Additionally, the Polynomial Intermediate Value Theorem \ref{real-ivt-poly} tells us that since $p'(-1)$ and $p'(2)$ have opposite signs, the derivative $p'(x)$ must have at least one root between $-1$ and $2$. This feels like a hint of an inductive procedure: perhaps we can count the number of real roots of $p'(x)$ between $-1$ and $2$, use that to count the number of local minima and maxima of $p(x)$, etc.?

As it turns out, the Sturm chain trick doesn't require us to count the number of real roots of the derivative $p'(x)$. It is entirely based on rewriting the computations we made during the Euclidean algorithm, when we checked that $p(x)$ and $p'(x)$ have no common factor, in the following equivalent form:
\begin{align*}
3\cdot(x^3 - 6x^2 + 4x + 12) + 4\cdot(4x-11) &= (x-2)\cdot(3x^2 - 12x + 4),\\
16\cdot(3x^2 - 12x + 4) + 101\cdot 1 &= (12x-15)\cdot(4x-11).
\end{align*}
Introducing the notation
\begin{align*}
p_0(x) &= p(x) = x^3 - 6x^2 + 4x + 12,\\
p_1(x) &= p'(x) = 3x^2 - 12x + 4,\\
p_2(x) &= 4x - 11,\\
p_3(x) &= 1,
\end{align*}
we see that for each $i = 1,2$ we have an equation of the form
\[
\text{positive}\cdot p_{i-1}(x) + \text{positive}\cdot p_{i+1}(x) = \text{something}\cdot p_i(x).
\]
In particular, we have
\[
p_i(x) = 0 \;\;\; \implies \;\;\; p_{i-1}(x) \text{ and } p_{i+1}(x) \text{ have opposite signs.}
\]
Since the gcd of any consecutive pair $\gcd(p_i(x), p_{i+1}(x))$ is $1$, when $p_i(x) = 0$ we don't have to worry about the possibility of $p_{i-1}(x)$ or $p_{i+1}(x)$ also being $0$, so the statement above is always meaningful.

Now let's see what happens to the signs of our four polynomials $p_0(x), p_1(x), p_2(x), p_3(x)$ as $x$ goes from $-\infty$ to $+\infty$. For now, we will cheat by using a computer to find all of the roots of these polynomials: the roots of $p_0(x)$ occur at
\[
-1.05..., 2.51..., 4.53...,
\]
the roots of $p_1(x)$ occur at
\[
0.36..., 3.63...,
\]
and the root of $p_2(x)$ occurs at
\[
2.75.
\]
Armed with these numerical computations, together with the fact that the polynomials $p_i(x)$ can't change sign without passing through a $0$ (by the Polynomial Intermediate Value Theorem \ref{real-ivt-poly}), we can make the following table:
\[
\begin{array}{c | c | c | c | c | c | c | c | c | c | c | c | c | c}
 & \cdots & -1.05 & \cdots & 0.36 & \cdots & 2.51 & \cdots & 2.75 & \cdots & 3.63 & \cdots & 4.53 & \cdots\\
\hline
p_0 & - & 0 & + & + & + & 0 & - & - & - & - & - & 0 & +\\
\hline
p_1 & + & + & + & 0 & - & - & - & - & - & 0 & + & + & +\\
\hline
p_2 & - & - & - & - & - & - & - & 0 & + & + & + & + & +\\
\hline
p_3 & + & + & + & + & + & + & + & + & + & + & + & + & +
\end{array}
\]
What do we notice when we stare at this table? Visually, the $-$ and $+$ signs seem to ``flow around'' the $0$s in the table, and the whole pattern of $+$ and $-$ signs seems to simplify as we move from left to right. More precisely, the pattern seems to simplify \emph{exactly when we pass through a root of } $p_0(x) = p(x)$!

Before concluding too much from this example, let's try another example, with a cubic polynomial that only has one real root. Suppose we take
\[
p(x) = x^3 - x + 1,
\]
and we want to know how many roots $p(x)$ has and where they are. Once again, we compute the gcd of $p(x)$ and $p'(x) = 3x^2 - 1$, but we arrange our computation in the slightly weird way we did before:
\begin{align*}
3\cdot(x^3 - x + 1) + 1\cdot(2x-3) &= x\cdot(3x^2 - 1)\\
4\cdot(3x^2-1) + 23\cdot (-1) &= (6x+9)\cdot(2x-3).
\end{align*}
Defining $p_0, p_1, p_2, p_3$ by
\begin{align*}
p_0(x) &= p(x) = x^3 - x + 1,\\
p_1(x) &= p'(x) = 3x^2 - 1,\\
p_2(x) &= 2x - 3,\\
p_3(x) &= -1,
\end{align*}
we see once again that for each $i = 1,2$ we have an equation of the form
\[
\text{positive}\cdot p_{i-1}(x) + \text{positive}\cdot p_{i+1}(x) = \text{something}\cdot p_i(x),
\]
so
\[
p_i(x) = 0 \;\;\; \implies \;\;\; p_{i-1}(x) \text{ and } p_{i+1}(x) \text{ have opposite signs.}
\]
The single real root of $p_0(x)$ is located at $-1.32..$, the roots of $p_1(x)$ are located at $\pm 0.57...$, and the root of $p_2(x)$ is located at $1.5$. Making a table as before, we get:
\[
\begin{array}{c | c | c | c | c | c | c | c | c | c}
& \cdots & -1.32 & \cdots & -0.57 & \cdots & 0.57 & \cdots & 1.5 & \cdots\\
\hline
p_0 & - & 0 & + & + & + & + & + & + & +\\
\hline
p_1 & + & + & + & 0 & - & 0 & + & + & +\\
\hline
p_2 & - & - & - & - & - & - & - & 0 & +\\
\hline
p_3 & - & - & - & - & - & - & - & - & -
\end{array}
\]
Once again, the $+$ and $-$ signs appear to flow around the $0$s in the table, and the pattern of $+$ and $-$ signs simplifies as we move past the single root of $p_0(x) = p(x)$.

Why does this happen? Well, as long as
\[
p_i(x) = 0 \;\;\; \implies \;\;\; p_{i-1}(x) \text{ and } p_{i+1}(x) \text{ have opposite signs,}
\]
if we zoom in around a root $r$ of the polynomial $p_i(x)$, we will always see either the pattern
\[
\begin{array}{c | c | c | c}
& \cdots & r & \cdots\\
\hline
p_{i-1} & + & + & +\\
\hline
p_i & ? & 0 & ?\\
\hline
p_{i+1} & - & - & -
\end{array}
\]
or the pattern
\[
\begin{array}{c | c | c | c}
& \cdots & r & \cdots\\
\hline
p_{i-1} & - & - & -\\
\hline
p_i & ? & 0 & ?\\
\hline
p_{i+1} & + & + & +
\end{array}
\]
and regardless of how the $?$s are filled in with $+$ or $-$ signs, the sequence of signs of $p_{i-1}(x), p_i(x), p_{i+1}(x)$ will flip from $+$ to $-$ (or vice versa) exactly once for any $x \approx r$.

That explains what is going on around the internal $0$s of the table - how about the $0$s at the top? The claim is that every time $p_0(x) = p(x)$ passes through the value $0$, the pattern of $+$ and $-$ signs always simplifies as we go from left to right, and never becomes more complex. To see why this is true, we need to use the fact that $p_1(x) = p'(x)$ is the \emph{derivative} of $p(x)$:
\begin{itemize}
\item if $p(r) = 0$ and $p'(r) > 0$, then $p(x)$ is increasing for $x \approx r$, so for $x$ just below $r$ we have $p(x) < 0$, and for $x$ just above $r$ we have $p(x) > 0$, while

\item if $p(r) = 0$ and $p'(r) < 0$, then $p(x)$ is decreasing for $x \approx r$, so for $x$ just below $r$ we have $p(x) > 0$, and for $x$ just above $r$ we have $p(x) < 0$.
\end{itemize}
If we zoom in around the root $r$ of $p(x)$, in the first case we see the pattern
\[
\begin{array}{c | c | c | c}
& \cdots & r & \cdots\\
\hline
p_0 & - & 0 & +\\
\hline
p_1 & + & + & +
\end{array}
\]
while in the second case we see the pattern
\[
\begin{array}{c | c | c | c}
& \cdots & r & \cdots\\
\hline
p_0 & + & 0 & -\\
\hline
p_1 & - & - & -
\end{array}
\]
and in either case, the signs of $p_0(x), p_1(x)$ are different for $x$ just below $r$ and are the same for $x$ just above $r$.

Now we formalize what we have discovered.

\begin{defn} If $a_1, ..., a_k$ is a sequence of numbers in a real closed field, then we define the number of \emph{sign changes} in the sequence, written $\operatorname{sc}(a_1, ..., a_k)$, to be one less than the maximum length of a sequence of indices $1 \le i_0 < i_2 < \cdots < i_s \le k$ such that
\[
a_{i_j} \cdot a_{i_{j+1}} < 0
\]
for all $j < s$.
\end{defn}

\begin{thm}[Sturm]\label{thm-sturm} Suppose a polynomial $p(x)$ with coefficients from a real closed field has no common factor with its derivative $p'(x)$, and we have a sequence of polynomials $p_0(x), ..., p_k(x)$ such that
\begin{itemize}
\item the polynomial $p_0(x)$ always has the same sign as $p(x)$,

\item the polynomial $p_1(x)$ always has the same sign as $p'(x)$,

\item for each $1 \le i < k$, if $p_i(x) = 0$ then $p_{i-1}(x)$ and $p_{i+1}(x)$ have opposite signs (and are nonzero), and

\item the polynomial $p_k(x)$ has a constant (nonzero) sign.
\end{itemize}
Then for any $a \le b$ (including $\pm\infty$), the number of roots of $p(x)$ in the half-open interval $(a,b]$ is exactly
\[
\operatorname{sc}\big(p_0(a), ..., p_k(a)\big) - \operatorname{sc}\big(p_0(b), ..., p_k(b)\big).
\]
\end{thm}

A sequence of polynomials $p_0, p_1, ..., p_k$ as in Theorem \ref{thm-sturm} is called a \emph{Sturm chain} for the polynomial $p(x)$. A slight variation of Theorem \ref{thm-sturm} lets us check whether a different polynomial $q(x)$ is positive at the roots of $p(x)$.

\begin{thm} Suppose that polynomials $p(x),q(x)$ with coefficients from a real closed field have no common factor, that $p(x)$ has no common factor with $p'(x)$, and that we have a sequence of polynomials $p_0(x), p_1(x), ..., p_k(x)$ as in Theorem \ref{thm-sturm} but with the condition on $p_1(x)$ modified to:
\begin{itemize}
\item the polynomial $p_1(x)$ always has the same sign as $p'(x) \cdot q(x)$.
\end{itemize}
Then for any $a \le b$ (including $\pm\infty$), the difference between the number of roots of $p(x)$ in the half-open interval $(a,b]$ where $q(x)$ is positive and the number of roots of $p(x)$ in the half-open interval $(a,b]$ where $q(x)$ is negative is exactly
\[
\operatorname{sc}\big(p_0(a), ..., p_k(a)\big) - \operatorname{sc}\big(p_0(b), ..., p_k(b)\big).
\]
\end{thm}

Let's try computing a Sturm chain for the generic cubic polynomial
\[
p(x) = x^3 - ax + b.
\]
We have $p'(x) = 3x^2 - a$, and our Euclidean algorithm computation goes as follows:
\begin{align*}
3\cdot(x^3 + ax + b) + 1\cdot(2ax - 3b) &= x\cdot(3x^2 - a),\\
4a^2\cdot(3x^2 - a) + 1\cdot(4a^3 - 27b^2) &= (6ax + 9b)\cdot(2ax - 3b),
\end{align*}
so we get the Sturm chain
\begin{align*}
p_0(x) &= p(x) = x^3 - ax + b,\\
p_1(x) &= p'(x) = 3x^2 - a,\\
p_2(x) &= 2ax - 3b,\\
p_3(x) &= 4a^3 - 27b^2.
\end{align*}
In particular, the number of real roots of $p(x)$ is given by
\[
\operatorname{sc}(-1, 3, -2a, 4a^3 - 27b^2) - \operatorname{sc}(1, 3, 2a, 4a^3 - 27b^2) = \begin{cases}3 & 4a^3 > 27b^2,\\ 1 & 4a^3 < 27b^2.\end{cases}
\]

On top of being useful for proving one-variable polynomial inequalities, Sturm chains can be useful for numerically approximating the real roots of a given polynomial $p(x)$:
\begin{itemize}
\item first, use Proposition \ref{prop-poly-multiplicity} to split $p(x)$ into a product of powers of polynomials which have no repeated roots, and have no roots in common with each other,

\item second, supposing for simplicity that $p(x)$ has no repeated roots, use the Euclidean algorithm to compute a Sturm chain $p_0, p_1, ..., p_k$ for $p(x)$,

\item third, use Theorem \ref{thm-sturm} to count the number of real roots in $(-\infty, +\infty]$, and repeatedly chop this half-open interval into pieces using something like binary search until we get a sequence of half-open intervals $(-\infty, a_1], (a_1, a_2], ..., (a_m, +\infty]$ which each contain at most one root of $p(x)$,

\item fourth, for each interval $(a_i,a_{i+1}]$ containing one of the roots $r$ of $p(x)$, start with a decent approximation to $r$ and repeatedly refine it using Newton's method until you have computed $r$ to as many digits of precision as desired.
\end{itemize}
In practice, there are faster ways to numerically compute the real roots of a one-variable polynomial, based on variations of Descartes' rule of signs (which only bounds the number of roots in an interval - but this is sometimes good enough). One convenient way to rephrase Descartes' rule of signs is to use the fact that the coefficients of a polynomial $p(x)$ are equal to the (higher) derivatives
\[
p(0), p'(0), p''(0)/2, ..., p^{(\deg p)}(0)/(\deg p)!,
\]
where $p^{(k)}$ is an abbreviation for the $k$th derivative of $p$.

\begin{thm}[Fourier-Budan Theorem]\label{thm-fourier-budan} If $p(x)$ is a polynomial of degree $d$ with coefficients from a real closed field, then for any $a \le b$, there is a whole number $k \ge 0$ such that the number of roots of $p(x)$ in the interval $(a,b]$, counted ``with multiplicity'', is given by
\[
\operatorname{sc}\big(p(a), p'(a), ..., p^{(d)}(a)\big) - \operatorname{sc}\big(p(b), p'(b), ..., p^{(d)}(b)\big) - 2k.
\]
\end{thm}

The proof of the Fourier-Budan Theorem \ref{thm-fourier-budan} is similar to the proof of Sturm's Theorem \ref{thm-sturm} - we just analyze how the number of sign changes in the sequence $p(x), ..., p^{(d)}(x)$ changes as $x$ passes through a root of $p(x)$ or a root of some $p^{(i)}(x)$.

There is a clever trick we can use to deal with the fact that the Fourier-Budan Theorem \ref{thm-fourier-budan} sometimes gives an overestimate of the number of roots in an interval $(a,b]$. Consider once again the cubic polynomial
\[
p(x) = x^3 - x + 1
\]
which we studied before. If we apply the Fourier-Budan Theorem \ref{thm-fourier-budan} to try to bound the number of roots in the interval $(0,1]$, we get
\begin{align*}
\operatorname{sc}\big(p(0), p'(0), p''(0), p^{(3)}(0)\big) - \operatorname{sc}\big(p(1), p'(1), p''(1), p^{(3)}(1)\big) &= \operatorname{sc}\big(1, -1, 0, 6\big) - \operatorname{sc}\big(1, 2, 6, 6\big)\\
&= 2 - 0,
\end{align*}
even though $p(x)$ has no positive roots. To probe the interval $(0,1]$, the trick is to make the change of variables
\[
x = \frac{1}{1+y},
\]
so that $x \in (0,1]$ exactly when $y \ge 0$. We then define a new polynomial $q(y)$ by
\begin{align*}
q(y) &= (1+y)^3 p\Big(\frac{1}{1+y}\Big)\\
&= 1 - (1+y)^2 + (1+y)^3\\
&= y^3 + 2y^2 + y + 1.
\end{align*}
Descartes' rule of signs then shows that $q(y)$ has no positive roots, so we see that $p(x)$ has no roots in the interval $(0,1]$. In some cases, it may be necessary to make a sequence of several changes of variables
\[
x_i = a_i + \frac{1}{1 + x_{i+1}}
\]
in order to rule out the spurious ``roots'' suggested by the Fourier-Budan Theorem \ref{thm-fourier-budan} - and this approach naturally leads to a procedure that finds continued fraction approximations of the actual roots which remain. In order to understand when these changes of variables will finally get rid of all of the extra sign changes, we use the following result.

\begin{thm}[Vincent's Theorem] Suppose that $p(x)$ has real coefficients, let $a < b$ be real numbers, and define a polynomial $q(y)$ by
\[
q(y) = (y+1)^{\deg p}p\Big(\frac{ay + b}{y + 1}\Big).
\]
Then:
\begin{itemize}
\item if $p(x)$ has no real roots in the interval $(a,b)$, and has no complex roots in the disk
\[
\Big|x - \frac{a+b}{2}\Big| < \frac{b-a}{2},
\]
then $q(y)$ has nonnegative coefficients, and

\item if $p(x)$ has exactly one real root (counted with multiplicity) in the interval $(a,b)$, and has no other complex roots in the disk
\[
\Big|x - \frac{a+b}{2} - i\frac{b-a}{2\sqrt{3}}\Big| < \frac{b-a}{\sqrt{3}},
\]
or in its complex conjugate, then the sequence of coefficients of $q(y)$ has exactly one sign change.
\end{itemize}
\end{thm}

This is a consequence of the following two results, applied to $q(y)$. The first follows directly from the Fundamental Theorem of Algebra, while the second is a bit harder.

\begin{prop} If $p(x)$ has real coefficients, and if none of the (possibly complex) roots of $p(x)$ have a positive real part, then all of the coefficients of $p(x)$ are nonnegative.
\end{prop}

\begin{thm}[Obreschkoff's Cone Theorem] If $p(x)$ has real coefficients and has exactly one positive real root (counted with multiplicity), and if every other root $a+bi$ of $p(x)$ satisfies
\[
a\sqrt{3} + |b| \le 0,
\]
then the sequence of coefficients of $p(x)$ has exactly one sign change.
\end{thm}
\begin{proof}[Proof sketch] Write $p(x) = (x-r)q(x)$. Then by the Fundamental Theorem of Algebra, $q(x)$ can be written as a product of linear and quadratic factors corresponding to roots of $p(x)$. If $a+bi$ is a complex root of $p(x)$, then the corresponding quadratic factor of $q(x)$ is
\[
(x-a)^2 + b^2 = x - 2ax + a^2 + b^2.
\]
If $a \le 0$, then the inequality $a\sqrt{3} + |b| \le 0$ is equivalent to
\[
1\cdot(a^2 + b^2) \le (2a)^2,
\]
so the sequence of coefficients of each quadratic factor of $q(x)$ are \emph{log-concave}, where we say that a sequence of positive numbers $a_0, ..., a_n$ is log-concave when we have
\[
a_{i-1}a_{i+1} \le a_i^2
\]
for all $i = 1, ..., n-1$. It's a standard (and slightly tricky to prove) fact that a product of two polynomials with positive, log-concave sequences of coefficients will itself have a positive, log-concave sequence of coefficients, so we can conclude that the sequence of coefficients of $q(x)$ is also positive and log-concave.%TODO: should I prove that products of log-concave polynomials are log-concave? It follows from the identity (sum_i a_ib_{-i})^2 - (\sum_i a_ib_{1-i})(\sum_i a_{i+1}b_{-i}) = \sum_{i \le j} (a_ia_j - a_{i-1}a_{j+1})(b_ib_j - b_{i-1}b_{j+1}), from the paper "On The Product Of Log-Concave Polynomials" by Woong Kook

To finish, we just check that if $r$ is positive and if the coefficients of $q(x)$ form a positive and log-concave sequence, then the sequence of coefficients of $p(x) = (x-r)q(x)$ has exactly one sign change.
\end{proof}

If we just want to study polynomials which are positive on some fixed half-open interval $(a,b]$, then there is a way to convert this to the simpler problem of studying polynomials which are positive everywhere, based on a similar change of variables. We use the fact that the rational function
\[
\phi_{a,b} : y \mapsto \frac{ay^2 + b}{y^2 + 1}
\]
has
\[
\phi_{a,b}(\RR) = (a,b],
\]
so $p(x)$ is positive on the interval $(a,b]$ if and only if the polynomial
\[
q(y) = (y^2 + 1)^{\deg p} p\Big(\frac{ay^2 + b}{y^2 + 1}\Big)
\]
is positive for all $y \in \RR$. The collection of polynomials $q(y)$ which are positive everywhere has the following nice characterization, based on the Fundamental Theorem of Algebra (which has a generalization that applies to every real closed field).

\begin{thm} A single variable polynomial $p(x)$ with coefficients in a real closed field satisfies the inequality
\[
p(x) \ge 0
\]
for all $x$ if and only if there are polynomials $f(x)$ and $g(x)$ with coefficients in the same real closed field which satisfy
\[
p(x) = f(x)^2 + g(x)^2.
\]

If $p$ has degree $2k$, then this occurs if and only if there is a multivariable quadratic polynomial $Q(x_0, x_1, ..., x_k)$ such that
\[
Q(x_0, x_1, ..., x_k) \ge 0
\]
for all $x_0, x_1, ..., x_k$ and
\[
p(x) = Q(1, x, ..., x^k).
\]
\end{thm}
\begin{proof} By the Fundamental Theorem of Algebra, there are numbers $r_i$ and multiplicities $m_i$, pairs $(a_j, b_j)$ and multiplicities $n_j$, and a constant $c$ such that
\[
p(x) = c\prod_i (x - r_i)^{m_i} \prod_j \big((x-a_j)^2 + b_j^2\big)^{n_j}.
\]
If $p(x) \ge 0$ for all $x$, then each multiplicity $m_i$ must be even and $c$ must be positive, so $p(x)$ can be written as a product of expressions which can each be written as a sum of two squares. The formula
\[
(x^2 + y^2)(z^2 + w^2) = (xz - yw)^2 + (xw + yz)^2
\]
can then be used to show that $p(x)$ itself can be written as a sum of two squares.

Now for the second statement. Clearly if $p(x) = Q(1, x, ..., x^k)$ and $Q(x_0, ..., x_k) \ge 0$, then $p(x) \ge 0$. For the other direction, if
\[
p(x) = f(x)^2 + g(x)^2
\]
with
\[
f(x) = \sum_{i=0}^k a_i x^i
\]
and
\[
g(x) = \sum_{i=0}^k b_i x^i,
\]
then we can take
\[
Q(x_0, ..., x_k) = \Big(\sum_{i=0}^k a_ix_i\Big)^2 + \Big(\sum_{i=0}^k b_ix_i\Big)^2.\qedhere
\]
\end{proof}

For instance, the fourth degree polynomial
\[
p(x) = x^4 + ax^2 + bx + c
\]
is always $\ge 0$ if and only if there is a constant $d \ge 0$ such that the three-variable quadratic polynomial
\[
Q(x,y,z) = x^2 + (a-d)xz + dy^2 + byz + cz^2
\]
is always $\ge 0$. If $d > 0$, then this quadratic form is always $\ge 0$ as long as the determinant
\[
\det\begin{bmatrix} 1 & 0 & (a-d)/2\\ 0 & d & b/2\\ (a-d)/2 & b/2 & c\end{bmatrix} = cd - b^2/4 - d(a-d)^2/4
\]
is positive, that is, as long as
\[
\big(4c - (a-d)^2\big)d \ge b^2.
\]
Incidentally, if we find a $d > 0$ which gives equality in the inequality above, then we can use it to factor $p(x)$ into a product of two positive quadratic polynomials.


\section{Some notes on Olympiad inequalities}

Good online resources for Olympiad level inequalities and techniques include Kiran Kedlaya's $A < B$ and Mildorf's notes.

\subsection{Algebraic inequalities}

\subsubsection{How hard are inequalities?}

\begin{thm}[Artin] If $P\in \RR[x_1, ..., x_n]$ is a polynomial satisfying $P(x_1, ..., x_n) \ge 0$ for all $x_1, ..., x_n \in \RR$, then there is an integer $k$ and a collection of polynomials $Q_i, R_i \in \RR[x_1, ..., x_n]$, $i = 1, ..., k$, satisfying
\[
P = \sum_{i=1}^k \frac{Q_i^2}{R_i^2}.
\]
\end{thm}

\begin{rem} It is not always possible to write a nonnegative polynomial as a sum of squares of polynomials. One famous example, due to Motzkin, is the polynomial
\[
x^4y^2 + x^2y^4 - 3x^2y^2 + 1,
\]
which is easily seen to be positive by AM-GM, but is not a sum of squares of polynomials. Artin's theorem tells us that it is possible to write it as a sum of squares of rational functions, but unfortunately gives us no bounds on how large the denominators of those rational functions may need to be.
\end{rem}

Although Artin's Theorem does not tell us what type of denominators to look for, in most cases the simplest possible denominators will work.

\begin{thm}[Polya] If $F(x_1, ..., x_n)$ is a homogeneous polynomial such that $F(x_1, ..., x_n) > 0$ whenever $x_i \ge 0$ for $1\le i \le n$ and not all $x_i$ are equal to $0$, then there is a number $p$ such that every coefficient of $(x_1+\cdots +x_n)^pF(x_1, ..., x_n)$ is positive.
\end{thm}

\begin{thm}[Tarski] There is an algorithm that can decide in a bounded amount of time the truth or falsity of any statement about real numbers built up from arithmetic operations, logical connectives, and logical quantifiers.
\end{thm}

\begin{rem} Unfortunately, Tarski's algorithm is very slow - his original algorithm's running time satisfied a recurrence similar to that of the Ackermann function. The modern form of this algorithm is called Cylindrical Algebraic Decomposition, and in the worst case the running time is doubly exponential in the size of the input. (This algorithm is implemented in many computer algebra programs, such as Mathematica.)
\end{rem}

\begin{thm}[Stengle's Positivstellansatz] Let $h_1, ..., h_k$ and $g_1, ..., g_l$ be polynomials in $\RR[x_1, ..., x_n]$. Then the set
\[
\{(x_1, ..., x_n)\in\RR^n\mid h_i(x_1, ..., x_n) = 0, g_j(x_1, ..., x_n) \ge 0 \mbox{ for } 1\le i \le k, 1\le j \le l\}
\]
is empty if and only if there are polynomials $t_1, ..., t_k$ and finitely many tuples $(i_1, ..., i_m)$ and sums of squares $s_{i_1, ..., i_m}$ such that
\[
-1 = \sum_{i=1}^k t_ih_i + \sum_{(i_1, ..., i_m)} s_{i_1, ..., i_m}g_{i_1}g_{i_2}\cdots g_{i_m}.
\]
\end{thm}

\subsubsection{Examples}

\begin{prop}[Lagrange's identity] For any $x_1, ..., x_n$, $y_1, ..., y_n$ we have
\[
\Big(\sum_i x_i^2\Big)\Big(\sum_i y_i^2\Big) - \left(\sum_i x_iy_i\right)^2 = \sum_{i<j} (x_iy_j-x_jy_i)^2.
\]
\end{prop}

\begin{prop} If $a\ge b$ and $x,y\in \RR$, then
\[
x^{a+1}y^{b-1} + x^{b-1}y^{a+1} - x^ay^b - x^by^a = x^{b-1}y^{b-1}(x-y)^2(x^a + x^{a-1}y + \cdots + y^a).
\]
\end{prop}

\begin{prop}[AM-GM] For any $x_1, ..., x_n$, we have
\[
\frac{x_1^n + \cdots + x_n^n}{n} - x_1\cdots x_n = \frac{1}{2n!} \sum_{sym} (x_{n-1}-x_n)^2\Big(\sum_{j=0}^{n-2} x_1\cdots x_j(x_{n-1}^{n-2-j} + x_{n-1}^{n-2-j-1}x_n + \cdots + x_n^{n-2-j})\Big).
\]
\end{prop}

\subsubsection{Problems}
\begin{enumerate}
\item Write $x^6+y^6+z^6-3x^2y^2z^2$ as a sum of squares of polynomials.

\item Prove that $\frac{1}{2}x^2+y^2+1\ge xy+x$ by writing the difference of both sides as a sum of squares.

\item Prove that $x^4+y^4+z^2 \ge \sqrt{8}xyz$ by writing the difference of both sides as a sum of squares.

%\item Prove that any polynomial $P(x)\in \RR[x]$ satisfying $P(x)\ge 0$ for all $x\in \RR$ can be written as a sum of two squares of polynomials.

\item (Mitrinovi\'c) Show that if $0 < b \le a$ then we have
\[
\frac{1}{8}\frac{(a-b)^2}{a} \le \frac{a+b}{2}-\sqrt{ab} \le \frac{1}{8}\frac{(a-b)^2}{b}.
\]

\item Show that
\[
\sqrt{a^2+1}^3 \ge a^3 + \frac{6}{5}a + \frac{3}{5}.
\]

\item Show that if $n\ge 2$ and $x_1, ..., x_n$ are positive real numbers satisfying
\[
\Big(\sum_i x_i\Big)\Big(\sum_i \frac{1}{x_i}\Big) \le n^2+1,
\]
then $\frac{x_1}{x_2} \le \frac{3+\sqrt{5}}{2}$.

\item For $a,b,c>0$ show that
\[
\frac{a^2}{b^2}+\frac{b^2}{c^2}+\frac{c^2}{a^2} \ge \frac{a}{b}+\frac{b}{c}+\frac{c}{a}.
\]

\item Show that
\[
\sum_{sym} x^4+3x^2y^2 \ge \sum_{sym} 4x^3y.
\]

\item Show that
\[
\sum_{sym} 3x^4+2x^2yz \ge \sum_{sym} 4x^3y+x^2y^2.
\]

\item Find a way to write the polynomial
\[
(x^2+y^2+1)(x^4y^2+x^2y^4-3x^2y^2+1)
\]
as a sum of squares.

\item (Nesbitt) Show that for $a,b,c > 0$, we have
\[
\frac{a}{b+c}+\frac{b}{c+a}+\frac{c}{a+b} \ge \frac{3}{2}.
\]

\item Prove that for $a,b,c > 0$ and $abc=1$, we have
\[
\sum_{cyc} \frac{ab}{a^5+ab+b^5} \le 1.
\]

\item (Schur) Show that for $x,y,z\ge 0$, we have
\[
\sum_{sym} x^3+xyz \ge \sum_{sym} 2x^2y.
\]

\item (Crux) Prove that if $a,b,c,d > 0$ and $c^2+d^2 = (a^2+b^2)^3$, then
\[
\frac{a^3}{c} + \frac{b^3}{d} \ge 1.
\]
What is the equality case?

\begin{comment}
% This is more of a function/derivative problem: can be solved by taking a_i = 1 + tx_i and showing that f(t) = \sqrt[n]{a_1\cdots a_n} has a positive third derivative (hint: take log/exp, rewrite derivative as exp(...) * poly of y_i = x_i/(1 + tx_i))
\item (Crux) Prove that for all $n$ and all $0 < a_1 \le \cdots \le a_n$, we have
\[
\frac{1}{2n^2}\frac{\sum_{i<j}(a_j-a_i)^2}{a_n} \le \frac{\sum_i a_i}{n}-\sqrt[n]{a_1\cdots a_n} \le \frac{1}{2n^2}\frac{\sum_{i<j}(a_j-a_i)^2}{a_1}.
\]
\end{comment}
\end{enumerate}

\subsection{Functional Inequalities}

\subsubsection{Useful facts about convex functions}

\begin{defn} A function $f$ is called \emph{convex} on the interval $I$ if for all $x,y\in I$ and $\lambda\in [0,1]$, we have
\[
f(\lambda x + (1-\lambda)y) \le \lambda f(x) + (1-\lambda)f(y).
\]
If $-f$ is convex, then $f$ is called \emph{concave}.
\end{defn}

\begin{thm} If $f$ has a derivative $f'$, then $f$ is convex if and only if $f'$ is increasing.
\end{thm}

\begin{thm}[Weighted Jensen] If $f$ is a convex function, $w_1, ..., w_n$ are positive real numbers, and $x_1, ..., x_n$ are any real numbers, then
\[
f\left(\frac{\sum_i w_ix_i}{\sum_i w_i}\right) \le \frac{\sum_i w_if(x_i)}{\sum_i w_i}.
\]
\end{thm}

\begin{prop} If $f$ is convex on the interval $[a,b]$ and $x \in [a,b]$, then
\[
f(x) \le \max(f(a),f(b)).
\]
\end{prop}

\begin{defn} If $a_1 \ge \cdots \ge a_n$ and $b_1 \ge \cdots b_n$, then we say that $(a_1, ..., a_n) \succ (b_1, ..., b_n)$, or $(a_1, ..., a_n)$ \emph{majorizes} $(b_1, ..., b_n)$, if
\begin{align*}
a_1 &\ge b_1,\\
a_1 + a_2 &\ge b_1+b_2,\\
&\cdots\\
a_1 + \cdots + a_{n-1} &\ge b_1 + \cdots + b_{n-1}, \mbox{ and}\\
a_1 + \cdots + a_n &= b_1 + \cdots + b_n.
\end{align*}
\end{defn}

\begin{thm}[Karamata] If $f$ is convex and $(a_1, ... a_n) \succ (b_1, ..., b_n)$, then
\[
\sum_{i=1}^n f(a_i) \ge \sum_{i=1}^n f(b_i).
\]
\end{thm}

\begin{thm}\label{n-1} If $f$ is convex on the interval $[\alpha,\infty)$ and concave on $(-\infty,\alpha]$, then for any whole number $n$ and any real numbers $x_1, ..., x_n$ we can find $a,b$ with $a+(n-1)b = x_1 + \cdots + x_n$ and
\[
f(x_1) + \cdots + f(x_n) \le f(a) + (n-1)f(b).
\]
\end{thm}

\subsubsection{Problems}
\begin{enumerate}
\item Show that if $f$ is convex, $a\le c$, and $b\le d$, then
\[
\frac{f(b)-f(a)}{b-a} \le \frac{f(d)-f(c)}{d-c}.
\]

\item Prove that each of the functions $|x|, x^2, e^x$ is convex, and that each of $\frac{1}{x}, -\sqrt{x}, -\ln(x)$ is convex when restricted to $x>0$, without using differential calculus.

\item Prove that if $f$ is convex, then for any $x,y,z$ we have
\[
f(4x) + f(4y) + f(4z) \ge f(2x+y+z) + f(x+2y+z) + f(x+y+2z).
\]

\item (Popoviciu) Prove that if $f$ is a convex function, then for any $x,y,z$ we have
\[
f(x) + f(y) + f(z) + 3f\left(\frac{x+y+z}{3}\right) \ge 2f\left(\frac{x+y}{2}\right) + 2f\left(\frac{y+z}{2}\right) + 2f\left(\frac{z+x}{2}\right).
\]

\item Prove Karamata's inequality.

\item Prove Theorem \ref{n-1}.

\item Prove that if $a,b,c \in [0,1]$, we have
\[
\frac{a}{b+c-1}+\frac{b}{c+a-1}+\frac{c}{a+b-1}+(1-a)(1-b)(1-c) \le 1.
\]

\item Prove that if one tetrahedron is contained in another tetrahedron, then the sums of the lengths of the edges of the inner tetrahedron is at most $\frac{4}{3}$ as large as the sum of the lengths of the edges of the outer tetrahedron. (Hint: the distance between two points is a convex function of either point.)

\item Prove that if $n$ is a whole number and $x_1, ..., x_n$ satisfy $x_1\cdots x_n = 1$, then
\[
\frac{1}{n-1+x_1} + \cdots + \frac{1}{n-1+x_n} \le 1.
\]

\begin{comment}
% duplicate problems
\item Prove that if $f$ is differentiable and $f'$ is convex, then we have
\begin{align*}
f(3)+3f(1) \ge&\ 3f(2) + f(0)\mbox{, and}\\
f(6)+f(2)+f(1) \ge&\ f(5) + f(4) + f(0).
\end{align*}

\item (Vasc) Prove that if $f$ is differentiable and $f'$ is convex, then for any $x\ge y\ge z$ we have
\[
f(2x+y)+f(2y+z)+f(2z+x) \ge f(2x+z) + f(2z+y) + f(2y+x).
\]

\item Popoviciu defines the divided differences of a polynomial inductively, as follows:
\begin{align*}
[a;f] =&\ f(a),\\
[a,b;f] =&\ \frac{f(b)-f(a)}{b-a},\\
[a_0,...,a_n;f] =&\ \frac{[a_1,...,a_n;f]-[a_0, ..., a_{n-1};f]}{a_n-a_0}.
\end{align*}
Prove, by induction on $n$, that if the $n$th derivative of $f$ exists and is nonnegative, that for any $a_0, ..., a_n$ we have
\[
[a_0, ..., a_n;f]\ge 0.
\]
\end{comment}
\end{enumerate}

\subsection{The Equally Moving Variables technique}

For more about this technique, see Pham Kim Hung's posts on the Art of Problem Solving forums. (His user name is hungkhtn.)

\begin{thm} Suppose $F:\RR^n\rightarrow\RR$ is an $n$-variable function satisfying the following:
\begin{itemize}
\item $F(x_1, x_2, ..., x_{n-1}, x_n) = F(x_2, x_3, ..., x_n, x_1)$ for all $x_1, ..., x_n$.
\item $F(x_1, ..., x_{n-1}, 0) \ge 0$ for all $x_1, ..., x_{n-1}\ge 0$.
\item $F(x_1+t, ..., x_n+t) \ge F(x_1, ..., x_n)$ for all $x_1, ..., x_n, t\ge 0$.
\end{itemize}
Then $F(x_1, ..., x_n) \ge 0$ for all $x_1, ..., x_n \ge 0$.
\end{thm}

It is often convenient to check the third condition with differential calculus. We define an operator $D$ by
\[
DF(x_1, ..., x_n) = \sum_{i=1}^n \frac{\partial}{\partial x_i}F(x_1, ..., x_n) = \frac{\partial}{\partial t}F(x_1+t, ..., x_n+t)\mid_{t=0}.
\]
To check the third condition, it is enough to check that $DF(x_1, ..., x_n)\ge 0$ for all $x_1, ..., x_n\ge 0$.

$D$ satisfies the Liebniz rule: if $F, G$ are two $n$-variable functions and $FG$ is their product, then
\[
D(FG)(x_1, ..., x_n) = F(x_1, ..., x_n)DG(x_1, ..., x_n) + G(x_1, ..., x_n)DF(x_1, ..., x_n).
\]
If $F(x_1,...,x_n) = (x_i-x_j)^k$ for a fixed $i,j,k$, then we have $DF(x_1, ..., x_n) = 0$.

\subsubsection{Problems}
\begin{enumerate}
\item (Schur's inequality) Prove, by induction on $r$, that for any $x,y,z\ge 0$ we have
\[
\sum_{sym} x^{r+2} + x^ryz \ge \sum_{sym} 2x^{r+1}y.
\]

\item (Pham Kim Hung) Prove that for $a,b,c\ge 0$ we have
\[
a^3+b^3+c^3-3abc \ge 4(a-b)(b-c)(c-a).
\]

\item Prove that for $x,y,z\ge 0$ we have
\[
\sum_{sym} 4x^4y^2 + 2x^3y^2z \ge \sum_{sym} 3x^4yz + 3x^3y^3.
\]

\item (Suranji) Prove, by induction on $n$, that for any $a_1, ..., a_n\ge 0$ we have
\[
(n-1)(a_1^n+\cdots +a_n^n)+na_1\cdots a_n \ge (a_1+\cdots +a_n)(a_1^{n-1}+\cdots +a_n^{n-1}).
\]

\item Prove that for $x,y,z\ge 0$ we have
\[
3(x^3+y^3+z^3) + 6xyz \ge 5(x^2y+y^2z+z^2x).
\]

\item Prove that for $x,y,z\ge 0$ we have
\[
\sum_{sym} x^9y^4 + x^7y^3z^3 \ge \sum_{sym} x^9y^3z + x^6y^6z.
\]
\end{enumerate}


\section{A few fewnomial exercises}

\begin{enumerate}
\item Prove that for $x \ge 0$, we have \[x^7 + x^4 + x^3 + 1 \ge 2x^6 + 2x.\]

\item Prove that for $x \ge 0$, we have \[x^{\sqrt{2}} + 2\sqrt{2} \ge 2^{\frac{3 - \sqrt{2}}{2}}x + 2.\]

\item Prove that for all $x$ we have \[3x^2e^x + 4e^2 \ge 8xe^x.\]

\item Prove that for $x \ge 0$, we have \[x^{22} + x^{11} + x^9 + 1 \ge x^{21} + x^{15} + x^4 + x^2.\]

\item Prove that for $x > 0$, we have \[x^{\sqrt{2}} + 2 + \frac{1}{x^{\sqrt{2}}} \ge 2x + \frac{2}{x}.\]

\item Prove that for $x \ge 0$, we have \[x^9 + 281x^3 + 100 \ge 22x^6 + 360x^2.\]

\item For $x, y > 0$, define their \emph{logarithmic mean} to be \[\text{LM}(x,y) = \frac{x-y}{\ln(x)-\ln(y)},\] where $\ln(x)$ is the natural logarithm of $x$. Prove that \[\frac{x+y}{2} \ge \text{LM}(x,y) \ge \sqrt{xy}.\]

\item Prove that for any $x > 0$ we have
\[
x^{66} + x^{29} + x^{26} + \frac{1}{x^{26}} + \frac{1}{x^{29}} + \frac{1}{x^{66}} \ge x^{62} + x^{45} + x^{2} + \frac{1}{x^{2}} + \frac{1}{x^{45}} + \frac{1}{x^{62}}.
\]

\item Prove that if $f$ is differentiable and $f'$ is convex, then we have
\begin{align*}
f(3)+3f(1) \ge&\ 3f(2) + f(0)\mbox{, and}\\
f(6)+f(2)+f(1) \ge&\ f(5) + f(4) + f(0).
\end{align*}

\item (Vasc) Prove that if $f$ is differentiable and $f'$ is convex, then for any $x\ge y\ge z$ we have
\[
f(2x+y)+f(2y+z)+f(2z+x) \ge f(2x+z) + f(2z+y) + f(2y+x).
\]

\item Popoviciu defines the divided differences of a polynomial inductively, as follows:
\begin{align*}
[a;f] =&\ f(a),\\
[a,b;f] =&\ \frac{f(b)-f(a)}{b-a},\\
[a_0,...,a_n;f] =&\ \frac{[a_1,...,a_n;f]-[a_0, ..., a_{n-1};f]}{a_n-a_0}.
\end{align*}
Prove, by induction on $n$, that if the $n$th derivative of $f$ exists and is nonnegative, that for any $a_0, ..., a_n$ we have
\[
[a_0, ..., a_n;f]\ge 0.
\]

\item Show that $[a_0, ..., a_n;f]$ is a symmetric function of $a_0, ..., a_n$.

\item Let $n$ be an integer which is at least $3$. Suppose $f$ is a function such that for every $a_0, ..., a_n$ we have $[a_0, ..., a_n; f] \ge 0$. Show that $f$ is differentiable and that for any $b_0, ..., b_{n-1}$ we have
\[
[b_0, ..., b_{n-1}; f'] \ge 0.
\]

\item Suppose that $f$ is a function such that for every integer $n$ and every $a_0, ..., a_n$ we have $[a_0, ..., a_n;f] \ge 0$. Prove that
\[
f(2) + 4f(0) \ge 4f(1).
\]

\item Prove that if $a_1, ..., a_9$ are real numbers satisfying $\sum_{i=1}^9 a_i = \sum_{i=1}^9 a_i^3 = 0$ and $\sum_{i=1}^9 a_i^2 = 8,$ then for any $x > 0$ we have
\[
x^2 + 7 + \frac{1}{x^2} \ge\ \sum_{i=1}^9 x^{a_i}\ \ge 4x + 1 + \frac{4}{x}.
\]

\item Prove that for any $x,y,z > 0$ we have
\[
\sum_{sym} \frac{x^2}{y^2} + \sum_{sym} \frac{x^{\sqrt{2}}}{y^{\sqrt{2}}}\ \ge\ \sum_{sym} \frac{x^2}{yz} + \sum_{sym} \frac{xy}{z^2}.
\]
\end{enumerate}


